# 🧠 AI_ML_Task 5: Decision Trees and Random Forests

## 📌 Internship Task Objective
Learn tree-based classification models and ensemble techniques using decision trees and random forests.

---

## 📂 Dataset
- **Source**: [Heart Disease Dataset – Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)
- **Format**: CSV (`heart.csv`)
- **Upload Method**: Manually uploaded to Google Colab

---

## 🛠️ Tools & Libraries
- Python
- Pandas, Scikit-learn
- Graphviz
- Google Colab

---

## ✅ Tasks Performed

1. Loaded and preprocessed the dataset
2. Trained a Decision Tree Classifier
3. Visualized the decision tree using Graphviz
4. Controlled tree depth to address overfitting
5. Trained a Random Forest Classifier
6. Compared accuracies between models
7. Evaluated model performance using 5-fold cross-validation
8. Interpreted feature importances

---

## 📈 Model Performance

| Model                    | Accuracy    |
|--------------------------|-------------|
| Decision Tree            | 0.98        |
| Pruned Decision Tree     | 0.8         |
| Random Forest            | 0.98        |
| Cross-Validation (RF)    | 0.99 (mean) |


---

## 🔍 Key Insights

- **Decision Trees** are interpretable but prone to overfitting.
- **Random Forests** improve generalization through bagging.
- **Tree depth control** and cross-validation are effective for preventing overfitting.
- **Feature importances** help identify the most influential variables.

---

## 📁 Files Included

- 'heart.csv' – Dataset
- 'Task5_DecisionTree_RandomForest.ipynb' – Main Colab notebook
- 'decision_tree.pdf' – Visualized tree output

---
