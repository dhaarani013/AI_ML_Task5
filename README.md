# ğŸ§  AI_ML_Task 5: Decision Trees and Random Forests

## ğŸ“Œ Internship Task Objective
Learn tree-based classification models and ensemble techniques using decision trees and random forests.

---

## ğŸ“‚ Dataset
- **Source**: [Heart Disease Dataset â€“ Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)
- **Format**: CSV (`heart.csv`)
- **Upload Method**: Manually uploaded to Google Colab

---

## ğŸ› ï¸ Tools & Libraries
- Python
- Pandas, Scikit-learn
- Graphviz
- Google Colab

---

## âœ… Tasks Performed

1. Loaded and preprocessed the dataset
2. Trained a Decision Tree Classifier
3. Visualized the decision tree using Graphviz
4. Controlled tree depth to address overfitting
5. Trained a Random Forest Classifier
6. Compared accuracies between models
7. Evaluated model performance using 5-fold cross-validation
8. Interpreted feature importances

---

## ğŸ“ˆ Model Performance

| Model                    | Accuracy    |
|--------------------------|-------------|
| Decision Tree            | 0.98        |
| Pruned Decision Tree     | 0.8         |
| Random Forest            | 0.98        |
| Cross-Validation (RF)    | 0.99 (mean) |


---

## ğŸ” Key Insights

- **Decision Trees** are interpretable but prone to overfitting.
- **Random Forests** improve generalization through bagging.
- **Tree depth control** and cross-validation are effective for preventing overfitting.
- **Feature importances** help identify the most influential variables.

---

## ğŸ“ Files Included

- 'heart.csv' â€“ Dataset
- 'Task5_DecisionTree_RandomForest.ipynb' â€“ Main Colab notebook
- 'decision_tree.pdf' â€“ Visualized tree output

---
